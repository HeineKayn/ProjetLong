{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import projetLib as proj\n",
    "import torch \n",
    "from math import sqrt,ceil\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "resize = (224,224)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "modelsaveFolder = \"./modelSave/\"\n",
    "folder_list = os.listdir(modelsaveFolder) \n",
    "\n",
    "def transformImg(img,resize,doRGB,doCrop):\n",
    "    if doCrop : res = proj.image.Crop_img(resize,doRGB)(img)\n",
    "    else : res = proj.image.Resize_img(resize,doRGB)(img)\n",
    "    return res\n",
    "    \n",
    "def extract_img(filepath):\n",
    "    with open(filepath, 'rb') as img_set:\n",
    "        img_arr = list(img_set.read())\n",
    "        sq   = ceil(sqrt(len(img_arr)))\n",
    "        rest = (sq*sq)-len(img_arr)\n",
    "        img_arr += [0]*rest\n",
    "        img_arr = np.array(img_arr)\n",
    "        img_arr = img_arr.astype('float32')\n",
    "        img_arr = np.reshape(img_arr, (sq,sq))\n",
    "        img_arr = Image.fromarray(img_arr.astype('uint8'), 'L')\n",
    "        return img_arr\n",
    "\n",
    "def predict(malwares,model_type,model_save,resize,doRGB,doCrop):\n",
    "    modelpath = modelsaveFolder + model_save + \"/\"\n",
    "    modelpath += os.listdir(modelpath)[-1]\n",
    "\n",
    "    inputchannels = 1\n",
    "    if doRGB: inputchannels = 3\n",
    "\n",
    "    model = None\n",
    "    if model_type == \"Basic\" : model = proj.model.Basic(inputchannels)\n",
    "    elif model_type == \"Resnet50\" : model = proj.model.getCNNresnet(50,inputchannels)\n",
    "    elif model_type == \"Resnet101\" : model = proj.model.getCNNresnet(101,inputchannels)\n",
    "    elif model_type == \"Resnet152\" : model = proj.model.getCNNresnet(152,inputchannels)\n",
    "    elif model_type == \"VGG\" : model = proj.model.VGG16(inputchannels)\n",
    "    model.load_state_dict(torch.load(modelpath,map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    images = []\n",
    "    tensors = []\n",
    "    for file in malwares:\n",
    "        img = extract_img(file.name)\n",
    "        img = transformImg(img,(resize,resize),doRGB,doCrop)\n",
    "        images.append(img)\n",
    "        tensors.append(transforms.ToTensor()(img))\n",
    "    tensors = torch.stack(tensors)\n",
    "    y = model(tensors)\n",
    "    return images,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTANT: You are using gradio version 3.0.22, however version 3.14.0 is available, please upgrade.\n",
      "--------\n",
      "Running on local URL:  http://127.0.0.1:7868/\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"900\" height=\"500\" allow=\"autoplay; camera; microphone;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"d:\\Documents\\Python\\lib\\site-packages\\gradio\\routes.py\", line 255, in run_predict\n",
      "    output = await app.blocks.process_api(\n",
      "  File \"d:\\Documents\\Python\\lib\\site-packages\\gradio\\blocks.py\", line 546, in process_api\n",
      "    predictions, duration = await self.call_function(fn_index, processed_input)\n",
      "  File \"d:\\Documents\\Python\\lib\\site-packages\\gradio\\blocks.py\", line 461, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"d:\\Documents\\Python\\lib\\site-packages\\anyio\\to_thread.py\", line 31, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"d:\\Documents\\Python\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 937, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"d:\\Documents\\Python\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 867, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"d:\\Documents\\Python\\lib\\site-packages\\gradio\\interface.py\", line 516, in <lambda>\n",
      "    else self.run_prediction(args)\n",
      "  File \"d:\\Documents\\Python\\lib\\site-packages\\gradio\\interface.py\", line 718, in run_prediction\n",
      "    prediction = predict_fn(*processed_input)\n",
      "  File \"C:\\Users\\thoma\\AppData\\Local\\Temp\\ipykernel_17444\\1098180630.py\", line 53, in predict\n",
      "    tensors.append(transformImg(img,(resize,resize),doRGB,doCrop))\n",
      "  File \"C:\\Users\\thoma\\AppData\\Local\\Temp\\ipykernel_17444\\1098180630.py\", line 16, in transformImg\n",
      "    if doCrop : res = proj.image.Crop_img(resize,doRGB)(img)\n",
      "  File \"d:\\Documents\\Python\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"d:\\Documents\\GitHub\\ProjetLong\\projetLib\\image.py\", line 128, in forward\n",
      "    else : return self.crop_img(img,self.size)\n",
      "  File \"d:\\Documents\\GitHub\\ProjetLong\\projetLib\\image.py\", line 108, in crop_img\n",
      "    img_arr = list(np.reshape(img_arr, (h2*w2)))\n",
      "MemoryError\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "demo = gr.Interface(\n",
    "    predict, \n",
    "\n",
    "    [gr.File(file_count=\"multiple\",label=\"Files to analyse\"),\n",
    "     gr.Dropdown([\"Resnet50\",\"Resnet101\",\"Resnet152\",\"VGG\",\"Basic\"],label=\"Model Types\"),\n",
    "     gr.Dropdown(folder_list,label=\"Model Saved\"),\n",
    "     gr.Number(value=\"224\",label=\"Resize value\"),\n",
    "     gr.Checkbox(label=\"RGB Image\"),\n",
    "     gr.Checkbox(label=\"Crop\")], \n",
    "\n",
    "    [gr.Gallery(label=\"Malwares en Image\"),\n",
    "     \"text\"]\n",
    "    # examples=[[[os.path.join(os.path.dirname(__file__),\"files/titanic.csv\"), \n",
    "    # os.path.join(os.path.dirname(__file__),\"files/titanic.csv\"), \n",
    "    # os.path.join(os.path.dirname(__file__),\"files/titanic.csv\")]]], \n",
    "    # cache_examples=True\n",
    "    )\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d27e3483a68ae9ee9392c1ae8c113dfd1d356d2ea26a62edcaca8c39a69f0c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
